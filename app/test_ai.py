from ai_assistant import load_local_model

tokenizer, model = load_local_model()

if model:
    prompt = (
        "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ª–∏—á–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.\n"
        "–ó–∞–¥–∞—á–∞: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–ø–ª–æ–º–∞.\n"
        "–û–ø–∏—Å–∞–Ω–∏–µ: –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∏–ø–ª–æ–º –ø–æ —Ç–µ–º–µ '–°–∞–π—Ç –¥–µ—Ä–µ–≤–µ–Ω—Å–∫–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤'.\n"
        "–î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π —Å–æ–≤–µ—Ç, –∫–∞–∫ –ª—É—á—à–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É.\n"
        "–°–æ–≤–µ—Ç:"
    )

    inputs = tokenizer(prompt, return_tensors="pt")

    output = model.generate(
        **inputs,
        max_new_tokens=60,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    text = tokenizer.decode(output[0], skip_special_tokens=True)

    if "–°–æ–≤–µ—Ç:" in text:
        text = text.split("–°–æ–≤–µ—Ç:")[-1].strip()

    text = text.replace("\n", " ").strip()
    if not text:
        text = "–ü–æ–ø—Ä–æ–±—É–π —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø—Ä–æ–µ–∫—Ç –Ω–∞ —ç—Ç–∞–ø—ã –∏ –Ω–∞—á–Ω–∏ —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞."

    print("üí° –°–æ–≤–µ—Ç –ò–ò:", text)
else:
    print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å.")
