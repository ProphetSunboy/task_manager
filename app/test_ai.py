from ai_assistant import load_local_model

tokenizer, model = load_local_model()

if model:
    # –Ø–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç + —è–≤–Ω—ã–π –º–∞—Ä–∫–µ—Ä –∫–æ–Ω—Ü–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    prompt = (
        "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ª–∏—á–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.\n"
        "–ó–∞–¥–∞—á–∞: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–ø–ª–æ–º–∞.\n"
        "–û–ø–∏—Å–∞–Ω–∏–µ: –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∏–ø–ª–æ–º –ø–æ —Ç–µ–º–µ '–°–∞–π—Ç –¥–µ—Ä–µ–≤–µ–Ω—Å–∫–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤'.\n"
        "–î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π —Å–æ–≤–µ—Ç, –∫–∞–∫ –ª—É—á—à–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É.\n"
        "–°–æ–≤–µ—Ç:"
    )

    inputs = tokenizer(prompt, return_tensors="pt")

    output = model.generate(
        **inputs,
        max_new_tokens=60,  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–æ–≤–µ—Ç–∞
        temperature=0.7,  # –º–µ–Ω—å—à–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
        top_p=0.9,  # –Ω–µ–º–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    text = tokenizer.decode(output[0], skip_special_tokens=True)

    # –û—Ç–¥–µ–ª—è–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –ø–æ—Å–ª–µ "–°–æ–≤–µ—Ç:"
    if "–°–æ–≤–µ—Ç:" in text:
        text = text.split("–°–æ–≤–µ—Ç:")[-1].strip()

    # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–π –º—É—Å–æ—Ä
    text = text.replace("\n", " ").strip()
    if not text:
        text = "–ü–æ–ø—Ä–æ–±—É–π —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø—Ä–æ–µ–∫—Ç –Ω–∞ —ç—Ç–∞–ø—ã –∏ –Ω–∞—á–Ω–∏ —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞."

    print("üí° –°–æ–≤–µ—Ç –ò–ò:", text)
else:
    print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å.")
